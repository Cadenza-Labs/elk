"""Functions for extracting the hidden states of a model."""

from ..utils import pytree_map
from .prompt_dataset import Prompt, PromptDataset, PromptConfig
from dataclasses import dataclass, InitVar
from einops import rearrange
from simple_parsing.helpers import field, Serializable
from torch.utils.data import DataLoader
from transformers import (
    BatchEncoding,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    AutoModel,
)
from typing import cast, Literal, Sequence, Iterable
import logging
import numpy as np
import torch
import torch.multiprocessing as mp
import warnings


@dataclass
class ExtractionConfig(Serializable):
    """
    Args:
        model: HuggingFace model string identifying the language model to extract
            hidden states from.
        prompts: The configuration for the prompt prompts.
        layers: The layers to extract hidden states from.
        layer_stride: Shortcut for setting `layers` to `range(0, num_layers, stride)`.
        token_loc: The location of the token to extract hidden states from. Can be
            either "first", "last", or "mean". Defaults to "last".
        use_encoder_states: Whether to extract hiddens from the encoder in
            encoder-decoder models. Defaults to False.
    """

    prompts: PromptConfig
    model: str = field(positional=True)

    layers: Sequence[int] = ()
    layer_stride: InitVar[int] = 1
    token_loc: Literal["first", "last", "mean"] = "last"
    use_encoder_states: bool = False

    def __post_init__(self, layer_stride: int):
        from transformers import AutoConfig, PretrainedConfig

        # Look up the model config to get the number of layers
        config = AutoConfig.from_pretrained(self.model)
        assert isinstance(config, PretrainedConfig)

        num_layers = getattr(config, "num_layers", config.num_hidden_layers)
        assert isinstance(num_layers, int)

        if self.layers and layer_stride > 1:
            raise ValueError(
                "Cannot use both --layers and --layer-stride. Please use only one."
            )
        elif layer_stride > 1:
            self.layers = list(range(0, num_layers, layer_stride))

        # TODO: Remove this once the extraction refactor is finished
        if self.layers and self.layers != list(range(num_layers)):
            warnings.warn(
                "Warning: hidden states are not labeled by layer index, and reporter "
                "checkpoints generated by `elk elicit` will be incorrectly named; "
                "e.g. `layer_1` instead of `layer_2` for the 3rd transformer layer "
                "when `--layer-stride` is 2. This will be fixed in a future release."
            )


def extract_hiddens(
    tokenizer: PreTrainedTokenizerBase,
    prompts: PromptDataset,
    config: ExtractionConfig,
    *,
    # TODO: Bring back auto-batching when we have a good way to prevent excess padding
    batch_size: int = 1,
) -> Iterable[tuple[torch.Tensor, list[int]]]:
    """Run inference on a model with a set of prompts, yielding the hidden states."""

    ctx = mp.get_context("spawn")
    queue = ctx.Queue()

    num_gpus = torch.cuda.device_count()

    # Spawn a process for each GPU
    ctx = torch.multiprocessing.spawn(
        _extract_hiddens_process,
        args=(num_gpus, queue, config, prompts, tokenizer, batch_size),
        nprocs=num_gpus,
        join=False,
    )
    assert ctx is not None

    # Yield results from the queue
    for _ in range(len(prompts)):
        yield queue.get()

    # Clean up
    ctx.join()


@torch.no_grad()
def _extract_hiddens_process(
    rank: int,
    world_size: int,
    queue: mp.Queue,
    config: ExtractionConfig,
    prompts: PromptDataset,
    tokenizer: PreTrainedTokenizerBase,
    batch_size: int,
):
    """
    Do inference on a model with a set of prompts on a single process.
    To be passed to Dataset.from_generator.
    """
    print(f"Process with rank={rank}")
    if rank != 0:
        logging.getLogger("transformers").setLevel(logging.CRITICAL)

    num_choices = prompts.num_classes
    shards = np.array_split(np.arange(len(prompts.active_split)), world_size)
    prompts.select_(shards[rank])

    # AutoModel should do the right thing here in nearly all cases. We don't actually
    # care what head the model has, since we are just extracting hidden states.
    model = AutoModel.from_pretrained(config.model, torch_dtype="auto").to(
        f"cuda:{rank}"
    )

    if config.use_encoder_states and not model.config.is_encoder_decoder:
        raise ValueError(
            "use_encoder_states is only compatible with encoder-decoder models."
        )

    # TODO: Make this configurable or something
    # Token used to separate the question from the answer
    sep_token = tokenizer.sep_token or "\n"

    # TODO: Maybe also make this configurable?
    # We want to make sure the answer is never truncated
    tokenizer.truncation_side = "left"
    if not tokenizer.pad_token:
        tokenizer.pad_token = tokenizer.eos_token

    def tokenize(strings: list[str]):
        return pytree_map(
            lambda x: x.to(f"cuda:{rank}"),
            tokenizer(
                strings,
                padding=True,
                return_tensors="pt",
                truncation=True,
            ),
        )

    # This function returns the flattened questions and answers, and the labels for
    # each question-answer pair. After inference we need to reshape the results.
    def collate(prompts: list[Prompt]) -> tuple[BatchEncoding, list[int], list[Prompt]]:
        choices = [
            prompt.to_string(i, sep=sep_token)
            for prompt in prompts
            for i in range(num_choices)
        ]
        return tokenize(choices), [prompt.label for prompt in prompts], prompts

    def collate_enc_dec(
        prompts: list[Prompt],
    ) -> tuple[BatchEncoding, BatchEncoding, list[int], list[Prompt]]:
        tokenized_questions = tokenize(
            [prompt.question for prompt in prompts for _ in range(num_choices)]
        )
        tokenized_answers = tokenize(
            [prompt.answers[i] for prompt in prompts for i in range(num_choices)]
        )
        labels = [prompt.label for prompt in prompts]
        return tokenized_questions, tokenized_answers, labels, prompts

    def reduce_seqs(
        hiddens: list[torch.Tensor], attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """Reduce sequences of hiddens into single vectors."""

        # Unflatten the hiddens
        hiddens = [rearrange(h, "(b c) l d -> b c l d", c=num_choices) for h in hiddens]

        if config.token_loc == "first":
            hiddens = [h[..., 0, :] for h in hiddens]
        elif config.token_loc == "last":
            # Because of padding, the last token is going to be at a different index
            # for each example, so we use gather.
            B, C, _, D = hiddens[0].shape
            lengths = attention_mask.sum(dim=-1).view(B, C, 1, 1)
            indices = lengths.sub(1).expand(B, C, 1, D)
            hiddens = [h.gather(index=indices, dim=-2).squeeze(-2) for h in hiddens]
        elif config.token_loc == "mean":
            hiddens = [h.mean(dim=-2) for h in hiddens]
        else:
            raise ValueError(f"Invalid token_loc: {config.token_loc}")

        if config.layers:
            hiddens = [hiddens[i] for i in config.layers]

        # [batch size, layers, num choices, hidden size]
        return torch.stack(hiddens, dim=1)

    # If this is an encoder-decoder model and we're passing the answer to the encoder,
    # we don't need to run the decoder at all. Just strip it off, making the problem
    # equivalent to a regular encoder-only model.
    is_enc_dec = model.config.is_encoder_decoder
    if is_enc_dec and config.use_encoder_states:
        # This isn't actually *guaranteed* by HF, but it's true for all existing models
        if not hasattr(model, "get_encoder") or not callable(model.get_encoder):
            raise ValueError(
                "Encoder-decoder model doesn't have expected get_encoder() method"
            )

        model = cast(PreTrainedModel, model.get_encoder())

    # Whether to concatenate the question and answer before passing to the model.
    # If False pass them to the encoder and decoder separately.
    should_concat = not is_enc_dec or config.use_encoder_states

    dl = DataLoader(
        prompts,
        batch_size=batch_size,
        collate_fn=collate if should_concat else collate_enc_dec,
    )

    # Iterating over questions
    for batch in dl:
        # Condition 1: Encoder-decoder transformer, with answer in the decoder
        if not should_concat:
            questions, answers, labels, prompts = batch
            outputs = model(
                **questions,
                **{f"decoder_{k}": v for k, v in answers.items()},
                output_hidden_states=True,
            )
            # [batch_size, num_layers, num_choices, hidden_size]
            # need to convert hidden states to numpy array first or
            # you get a ConnectionResetErrror
            hiddens = torch.stack(outputs.decoder_hidden_states, dim=2).cpu().numpy()

        # Condition 2: Either a decoder-only transformer or a transformer encoder
        else:
            choices, labels, prompts = batch

            # Skip the input embeddings which are unlikely to be interesting
            h = model(**choices, output_hidden_states=True).hidden_states[1:]

            # need to convert hidden states to numpy array first or
            # you get a ConnectionResetErrror
            hiddens = reduce_seqs(h, choices["attention_mask"]).cpu().numpy()

        # from_generator doesn't deal with batched output, so we split it up here
        for i in range(batch_size):
            queue.put(
                {
                    "hiddens": hiddens[i].astype(np.float16),
                    "layers": config.layers or list(range(len(hiddens[i]))),
                    "label": labels[i],
                    "answers": prompts[i].answers,
                    "template_name": prompts[i].template_name,
                    "text": prompts[i].to_string(0, sep=sep_token),  # arbitrary answer
                    "predicate": prompts[i].predicate,
                }
            )
