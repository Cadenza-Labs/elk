{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ConnardMcGregoire\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nnsight\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "import os\n",
    "\n",
    "from elk.extraction.prompt_loading import load_prompts\n",
    "\n",
    "os.environ['HF_TOKEN'] = \"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\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "#model_name = \"EleutherAI/pythia-12b\"\n",
    "#model_name = \"gpt2\"\n",
    "\n",
    "# first, load the state dict of the model from the model name\n",
    "\n",
    "model = nnsight.LanguageModel(model_name, device=\"cpu\")._load(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# check if tokenizer has a pad and bos token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.bos_token_id is None:\n",
    "    tokenizer.bos_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n",
      "        if self.config._attn_implementation == \"flash_attention_2\":\n",
      "            if attention_mask is not None and 0.0 in attention_mask:\n",
      "                return attention_mask\n",
      "            return None\n",
      "\n",
      "        dtype, device = input_tensor.dtype, input_tensor.device\n",
      "        min_dtype = torch.finfo(dtype).min\n",
      "        sequence_length = input_tensor.shape[1]\n",
      "        if hasattr(self.layers[0].self_attn, \"past_key_value\"):  # static cache\n",
      "            target_length = self.config.max_position_embeddings\n",
      "        else:  # dynamic cache\n",
      "            target_length = (\n",
      "                attention_mask.shape[-1] if isinstance(attention_mask, torch.Tensor) else cache_position[-1] + 1\n",
      "            )\n",
      "\n",
      "        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n",
      "        if sequence_length != 1:\n",
      "            causal_mask = torch.triu(causal_mask, diagonal=1)\n",
      "        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
      "        causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n",
      "        if attention_mask is not None:\n",
      "            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
      "            if attention_mask.dim() == 2:\n",
      "                mask_length = attention_mask.shape[-1]\n",
      "                padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n",
      "                causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n",
      "            elif attention_mask.dim() == 4:\n",
      "                # backwards compatibility: we allow passing a 4D attention mask shorter than the input length with\n",
      "                # cache. In that case, the 4D attention mask attends to the newest tokens only.\n",
      "                if attention_mask.shape[-2] < cache_position[0] + sequence_length:\n",
      "                    offset = cache_position[0]\n",
      "                else:\n",
      "                    offset = 0\n",
      "                mask_shape = attention_mask.shape\n",
      "                mask_slice = (attention_mask.eq(0.0)).to(dtype=dtype) * min_dtype\n",
      "                causal_mask[\n",
      "                    : mask_shape[0], : mask_shape[1], offset : mask_shape[2] + offset, : mask_shape[3]\n",
      "                ] = mask_slice\n",
      "\n",
      "        if (\n",
      "            self.config._attn_implementation == \"sdpa\"\n",
      "            and attention_mask is not None\n",
      "            and attention_mask.device.type == \"cuda\"\n",
      "        ):\n",
      "            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n",
      "            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n",
      "            # Details: https://github.com/pytorch/pytorch/issues/110213\n",
      "            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n",
      "\n",
      "        return causal_mask\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(model.model._update_causal_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialEmbedding(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.word_embed = model.model.wte\n",
    "        self.pos_embed = model.model.wpe\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # TODO : check that\n",
    "        pos_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0)\n",
    "        print(pos_ids)\n",
    "        return self.word_embed(input_ids) + self.pos_embed(pos_ids)\n",
    "\n",
    "def get_embed(model):\n",
    "    #return InitialEmbedding(model)\n",
    "    return model.model.embed_tokens\n",
    "\n",
    "def get_block(model, layer):\n",
    "    return model.model.layers[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_acts(statements, tokenizer, model, batch_size=32, layers=None, intermediate_device=\"cpu\", compute_device=DEVICE):\n",
    "    \"\"\"\n",
    "    Get given layer activations for the statements.\n",
    "    Return dictionary of stacked activations.\n",
    "\n",
    "    Caution: Layer 0 is embedding layer, layer 1 is the first transformer layer, so model.model.h[0]\n",
    "    \"\"\"\n",
    "    t_to_cpu = 0\n",
    "    t_to_gpu = 0\n",
    "    t_start = time.perf_counter_ns()\n",
    "    model.eval().to(intermediate_device)\n",
    "    t_to_cpu += time.perf_counter_ns() - t_start\n",
    "    if layers is None:\n",
    "        layers = list(range(model.config.num_hidden_layers + 1))\n",
    "\n",
    "    # get last token indexes for all statements\n",
    "    last_tokens = [len(tokenizer.encode(statement)) - 1 for statement in statements]\n",
    "\n",
    "    #print(last_tokens)\n",
    "\n",
    "    current_hiddens = []\n",
    "    cache_positions = []\n",
    "    positions_ids = []\n",
    "    all_hiddens = [[] for _ in range(model.config.num_hidden_layers + 1)]\n",
    "\n",
    "    t_start = time.perf_counter_ns()\n",
    "    embed = get_embed(model).to(compute_device)\n",
    "    t_to_gpu += time.perf_counter_ns() - t_start\n",
    "\n",
    "    bos_token = tokenizer.bos_token_id\n",
    "    \n",
    "    for batch_start in range(0, len(statements), batch_size):\n",
    "        batch = statements[batch_start:min(batch_start + batch_size, len(statements))]\n",
    "        # TODO : check for BOS token and last token (should be \".\")\n",
    "        input_ids = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "        t_start = time.perf_counter_ns()\n",
    "        input_ids = input_ids.to(compute_device)\n",
    "        t_to_gpu += time.perf_counter_ns() - t_start\n",
    "\n",
    "        if bos_token is not None and bos_token != input_ids[0, 0]:\n",
    "            input_ids = torch.cat([torch.zeros(input_ids.size(0), 1, device=input_ids.device, dtype=input_ids.dtype).fill_(bos_token), input_ids], dim=1)\n",
    "\n",
    "        # print(input_ids)\n",
    "        # print(input_ids.size())\n",
    "\n",
    "        current_hiddens.append(embed(input_ids))\n",
    "        cache_positions.append(torch.arange(input_ids.size(1), device=compute_device))\n",
    "        positions_ids.append(cache_positions[-1].unsqueeze(0))\n",
    "        if 0 in layers:\n",
    "            t_start = time.perf_counter_ns()\n",
    "            all_hiddens[0].append(current_hiddens[-1][torch.arange(input_ids.size(0)), last_tokens[batch_start:batch_start + input_ids.size(0)]].to(intermediate_device))\n",
    "            t_to_cpu += time.perf_counter_ns() - t_start\n",
    "        \n",
    "        t_start = time.perf_counter_ns()\n",
    "        input_ids = input_ids.to(intermediate_device)\n",
    "        t_to_cpu += time.perf_counter_ns() - t_start\n",
    "\n",
    "    t_start = time.perf_counter_ns()\n",
    "    embed.to(intermediate_device)\n",
    "    t_to_cpu += time.perf_counter_ns() - t_start\n",
    "\n",
    "    t_free = 0\n",
    "    t_start = time.perf_counter_ns()\n",
    "    torch.cuda.empty_cache()\n",
    "    t_free += time.perf_counter_ns() - t_start\n",
    "\n",
    "    #causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position)\n",
    "\n",
    "    for block_idx in range(max(layers)):\n",
    "        t_start = time.perf_counter_ns()\n",
    "        decoder_layer = get_block(model, block_idx).to(compute_device)\n",
    "        t_to_gpu += time.perf_counter_ns() - t_start\n",
    "\n",
    "        for batch_idx, batch in enumerate(current_hiddens):\n",
    "            cache_position = cache_positions[batch_idx]\n",
    "            position_ids = positions_ids[batch_idx]\n",
    "            causal_mask = model.model._update_causal_mask(None, batch, cache_position) \n",
    "            out = decoder_layer(\n",
    "                batch,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=None,\n",
    "                output_attentions=False,\n",
    "                use_cache=False,\n",
    "                cache_position=cache_position,\n",
    "            )[0]\n",
    "            if block_idx + 1 in layers:\n",
    "                idx = last_tokens[batch_idx * batch_size:batch_idx * batch_size + out.size(0)]\n",
    "                t_start = time.perf_counter_ns()\n",
    "                all_hiddens[block_idx + 1].append(\n",
    "                    out[\n",
    "                        torch.arange(out.size(0), device=out.device),\n",
    "                        idx\n",
    "                    ].to(intermediate_device)\n",
    "                )\n",
    "                t_to_cpu += time.perf_counter_ns() - t_start\n",
    "            current_hiddens[batch_idx] = out\n",
    "\n",
    "        t_start = time.perf_counter_ns()\n",
    "        decoder_layer.to(intermediate_device)\n",
    "        t_to_cpu += time.perf_counter_ns() - t_start\n",
    "        t_start = time.perf_counter_ns()\n",
    "        torch.cuda.empty_cache()\n",
    "        t_free += time.perf_counter_ns() - t_start\n",
    "    \n",
    "    print(f\"Time to CPU : {t_to_cpu / 1e9}\")\n",
    "    print(f\"Time to GPU : {t_to_gpu / 1e9}\")\n",
    "    print(f\"Time free : {t_free / 1e9}\")\n",
    "    \n",
    "    return {layer: torch.cat(acts) for layer, acts in enumerate(all_hiddens) if len(acts) > 0}\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_acts(\n",
    "    cfg,\n",
    "    layers=None,\n",
    "    split_type = \"train\",\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    output_dir=\"acts\",\n",
    "    noperiod=False,\n",
    "    intermediate_device=\"cpu\",\n",
    "    compute_device=DEVICE,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(cfg.model).to(intermediate_device)\n",
    "    \n",
    "    if layers is None:\n",
    "        layers = list(range(model.config.num_hidden_layers + 1))\n",
    "\n",
    "    ds_names = cfg.datasets\n",
    "\n",
    "    prompt_ds = load_prompts(\n",
    "        ds_names[0],\n",
    "        binarize=cfg.binarize,\n",
    "        num_shots=cfg.num_shots,\n",
    "        split_type=split_type,\n",
    "        template_path=cfg.template_path,\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "    \n",
    "    num_yielded = 0\n",
    "    for example_id, example in enumerate(prompt_ds):\n",
    "        num_variants = len(example[\"prompts\"])\n",
    "        num_choices = len(example[\"prompts\"][0])\n",
    "\n",
    "        hidden_dict = {\n",
    "            f\"hidden_{layer_idx}\": torch.empty(\n",
    "                num_variants,\n",
    "                num_choices,\n",
    "                model.config.hidden_size,\n",
    "                device=intermediate_device,\n",
    "                dtype=torch.int16,\n",
    "            )\n",
    "            for layer_idx in layers\n",
    "        }\n",
    "        \n",
    "        text_questions = []\n",
    "        statements = []\n",
    "        for i, record in enumerate(example[\"prompts\"]):\n",
    "            variant_questions = []\n",
    "\n",
    "            # Iterate over answers\n",
    "            for j, choice in enumerate(record):\n",
    "                text = choice[\"question\"]\n",
    "\n",
    "                variant_questions.append(\n",
    "                    dict(\n",
    "                        {\n",
    "                            \"template_id\": i,\n",
    "                            \"template_name\": example[\"template_names\"][i],\n",
    "                            \"text\": dict(\n",
    "                                {\n",
    "                                    \"question\": text,\n",
    "                                    \"answer\": choice[\"answer\"],\n",
    "                                }\n",
    "                            ),\n",
    "                            \"example_id\": example_id,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                statements.append(choice[\"question\"] + \" \" + choice[\"answer\"])\n",
    "            \n",
    "            text_questions.append(text)\n",
    "        \n",
    "        acts = get_acts(statements, tokenizer, model, layers=layers, intermediate_device=intermediate_device, compute_device=compute_device)\n",
    "\n",
    "        # Fill hidden_dict with activations\n",
    "        for layer_idx, act in acts.items():\n",
    "            idx = 0\n",
    "            for i, record in enumerate(example[\"prompts\"]):\n",
    "                for j, choice in enumerate(record):\n",
    "                    hidden_dict[f\"hidden_{layer_idx}\"][i, j] = act[idx]\n",
    "                    idx += 1\n",
    "            \n",
    "        # We skipped a variant because it was too long; move on to the next example\n",
    "        if len(text_questions) != num_variants:\n",
    "            continue\n",
    "\n",
    "        out_record = dict(\n",
    "            label=example[\"label\"],\n",
    "            variant_ids=example[\"template_names\"],\n",
    "            text_questions=text_questions,\n",
    "            **hidden_dict,\n",
    "        )\n",
    "\n",
    "        num_yielded += 1\n",
    "        yield out_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results :\n",
    "gpt2 \n",
    "- extract_hiddens : 6-7-8 min, 15 au début\n",
    "- generate_acts : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "c:\\Users\\ConnardMcGregoire\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:670: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to CPU : 4.3362326\n",
      "Time to GPU : 5.3387427\n",
      "Time free : 0.1654169\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "torch.Size([2, 4096])\n",
      "0 torch.Size([2, 4096])\n",
      "1 torch.Size([2, 4096])\n",
      "2 torch.Size([2, 4096])\n",
      "3 torch.Size([2, 4096])\n",
      "4 torch.Size([2, 4096])\n",
      "5 torch.Size([2, 4096])\n",
      "6 torch.Size([2, 4096])\n",
      "7 torch.Size([2, 4096])\n",
      "8 torch.Size([2, 4096])\n",
      "9 torch.Size([2, 4096])\n",
      "10 torch.Size([2, 4096])\n",
      "11 torch.Size([2, 4096])\n",
      "12 torch.Size([2, 4096])\n",
      "13 torch.Size([2, 4096])\n",
      "14 torch.Size([2, 4096])\n",
      "15 torch.Size([2, 4096])\n",
      "16 torch.Size([2, 4096])\n",
      "17 torch.Size([2, 4096])\n",
      "18 torch.Size([2, 4096])\n",
      "19 torch.Size([2, 4096])\n",
      "20 torch.Size([2, 4096])\n",
      "21 torch.Size([2, 4096])\n",
      "22 torch.Size([2, 4096])\n",
      "23 torch.Size([2, 4096])\n",
      "24 torch.Size([2, 4096])\n",
      "25 torch.Size([2, 4096])\n",
      "26 torch.Size([2, 4096])\n",
      "27 torch.Size([2, 4096])\n",
      "28 torch.Size([2, 4096])\n",
      "29 torch.Size([2, 4096])\n",
      "30 torch.Size([2, 4096])\n",
      "31 torch.Size([2, 4096])\n",
      "32 torch.Size([2, 4096])\n"
     ]
    }
   ],
   "source": [
    "acts = get_acts([\"Hello world.\", \"Hello hello !.\"], tokenizer, model, batch_size=2)\n",
    "\n",
    "for act in acts.values():\n",
    "    print(act.shape)\n",
    "\n",
    "for layer_idx, act in acts.items():\n",
    "    print(layer_idx, act.shape)\n",
    "\n",
    "\n",
    "# TODO : use nnsight to check the activations are the same\n",
    "\n",
    "#generate_acts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
